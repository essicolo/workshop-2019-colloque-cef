[
["index.html", "Analyse de données compositionnelles, biostatistiques et autoapprentissage avec R 1 L’analyse compositionnelle 1.1 Contexte 1.2 Les données compositionnelles 1.3 Les transformations compositionnelles 1.4 Les zéros", " Analyse de données compositionnelles, biostatistiques et autoapprentissage avec R Serge-Étienne Parent 2019-05-01 1 L’analyse compositionnelle 1.1 Contexte En 1898, le statisticien Karl Pearson nota que des corrélations étaient induites lorsque l’on effectuait des ratios par rapport à une variable commune. Figure 1.1: Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.—on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London Faisons l’exercice! Nous générons au hasard 1000 données (comme le proposait Pearson) pour trois dimensions: le fémur, le tibia et l’humérus. Ces dimensions ne sont pas générées par des distributions corrélées. library(&quot;tidyverse&quot;) set.seed(3570536) n &lt;- 1000 bones &lt;- tibble(femur = rnorm(n, 10, 3), tibia = rnorm(n, 8, 2), humerus = rnorm(n, 6, 2)) plot(bones) Figure 1.2: Représentation bivariées de longueur fictives des os, générées au hasard. cor(bones) ## femur tibia humerus ## femur 1.000000000 -0.069006171 0.002652292 ## tibia -0.069006171 1.000000000 -0.008994704 ## humerus 0.002652292 -0.008994704 1.000000000 Nous avons induit ce que Pearson appelait une fausse corrélation (spurious correlation). En 1960, Chayes proposa que ces fausses corrélations soient induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d’une somme totale. Par exemple, dans une composition simple de deux types d’utilisation du territoire, si une proportion augmente, l’autre doit nécessairement diminuer. n &lt;- 30 tibble(A = runif(n, 0, 1)) %&gt;% mutate(B = 1 - A) %&gt;% ggplot(aes(x=A, y=B)) + geom_point() + coord_equal() Figure 1.3: Complémentarité intrinsèque des données compositionnelles. 1.2 Les données compositionnelles Les variables exprimées relativement à une somme totale sont dites compositionnelles. Les variables compositionnelles peuvent être des concentrations, des décomptes, des dimensions (aires ou volumes), des masses, des dépenses monétaires: en fait, tout ce qui s’exprime en proportions d’un tout. Elles possèdent les caractéristiques suivantes. Redondance d’information. Un système de deux proportions ne contient qu’une seule variable du fait que l’on puisse déduire l’une en soutrayant l’autre de la somme totale. Un vecteur compositionnel contient de l’information redondante. Pourtant, effectuer des statistiques sur l’une plutôt que sur l’autre donnera des résultats différents. Dépendance d’échelle. Les statistiques devraient être indépendantes de la somme totale utilisée. Pourtant, elles différeront sur l’on utilise par exemple, une proportion des mâles d’une part et des femelles d’autre part, ou la proportion de la somme des deux, de même que les résultats d’un test sanguin différera si l’on utilise une base sèche ou une base humide. Distribution théorique des données. Étant donnée que les proportions sont confinées entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s’étend de -∞ à +∞) n’est souvent pas appropriée. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d’autres approches sont souvent plus pratiques. Pour illustrer l’effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l’argile. En utilisant des écart-types univariés, nous obtenons l’ellipse en rouge, qui non seulement représente peu l’étalement des données, mais elle dépasse les bornes du triangle, admettant ainsi des proportions négatives. En bleu, la distribution logistique normale (issue des méthodes présentées plus loin dans cette section) convient davantage. Figure 1.4: Représentation ternaire d’un écart-type multivarié biaisé (rouge) et non-biaisé (en bleu). Les conséquences d’effectuer des statistiques linéaires sur des données compositionnelles brutes peuvent être majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), s’appuyant en outre sur Rock (1988), note les problèmes suivants (exprimés en mes mots). les régressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification les propriétés des distributions peuvent être générées par l’opération de fermeture de la composition (s’assurer que le total des proportions donne 100%) les résultats d’analyses discriminantes linéaires sont propices à être illusoires tous les coefficients de corrélation seront affectés à des degrés inconnus les résultats des tests d’hypothèses seront intrinsèquement faussés Pour contourner ces problèmes, il faut d’abord aborder les données compositionnelles pour ce qu’elles sont: des données intrinsèquement multivariées. Elles sont un nuage de point multidimensionnel, et non pas une collection de variables individuelles. Ceci qui n’empêche pas d’effectuer des analyses consciencieusement sous des angles particuliers. Chargeons le module compositions (n’oubliez pas de l’installer au préalable) pour accéder à des données fictives de proportions de sable, limon et argile dans des sédiments. library(&quot;compositions&quot;) data(&quot;ArcticLake&quot;) ArcticLake &lt;- ArcticLake %&gt;% as_tibble() head(ArcticLake) ## # A tibble: 6 x 4 ## sand silt clay depth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77.5 19.5 3 10.4 ## 2 71.9 24.9 3.2 11.7 ## 3 50.7 36.1 13.2 12.8 ## 4 52.2 40.9 6.6 13 ## 5 70 26.5 3.5 15.7 ## 6 66.5 32.2 1.3 16.3 En R, on pourra aisément rapporter une composition en somme unitaire grâce à la fonction base::apply(). Pour information, la fonction base::apply() applique la fonction FUN = x/sum(x) pour toutes les lignes (MARGIN = 1, pour les colonnes, ce serait MARGIN = 2). Les résultats étant rendus sous forme de colonne, on doit finalement transposer (base:t()). comp &lt;- ArcticLake %&gt;% dplyr::select(-depth) %&gt;% apply(., MARGIN = 1, FUN = function(x) x/sum(x)) %&gt;% t(.) Plus simplement, pourra aussi utiliser la fonction compositions::acomp() (acomp pour Aitchison-composition) pour fermer la composition à une somme de 1. comp &lt;- ArcticLake %&gt;% dplyr::select(-depth) %&gt;% acomp(.) comp[1:6, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 ## [6,] 0.6650000 0.3220000 0.0130000 Cette stratégie a pour avantage d’attribuer à la variable comp la classe acomp, qui automatise les opérations dans l’espace compositionnel (que l’on nomme aussi le simplex). Notamment, la représentation ternaire est souvent utilisée pour présenter des compositions. Toutefois, il est difficile d’interpréter les compositions de plus de trois parties. La classe acomp automatise aussi la représentation teranaire. plot(comp) Figure 1.5: Représentation ternaire d’une composition granulométrique. 1.3 Les transformations compositionnelles Afin de transposer cet espace clôt entre 0 et 1 en un espace ouvert de \\(-\\infty\\) à \\(+\\infty\\), on pourra diviser chaque proportion par une proportion de référence choisie parmi n’importe quelle proportion. Du coup, on retire une dimension redondante! En utilisant le log du ratio, l’inverse du ratio ne sera qu’un changement de signe, ce qui est pratique en statistiques linéaries. Cette solution, proposée par Aitchison (1986), est valide peu importe le nombre d’éléments formant la compositions. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\): \\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\] Dans R, la colonne de référence est par défaut la dernière colonne de la matrice des compositions. add_lr &lt;- alr(comp) Cette dernière stratégie se nomme les log-ratios additifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratégie a le désavantage de dépendre de la décision arbitraire de la composante à utiliser au numérateur. Deuxième restriction des alr: les axes de l’espace des alr n’étant pas orthogonaux, ils ne peuvent pas être utilisés pour effectuer des statistiques basées sur les distances euclidiennes étant donnée que la distance entre les points dépend de la variable choisie comme dénominateur commun. add_lr_switch &lt;- alr(comp[, c(2, 3, 1)]) dist(unclass(add_lr[1:6, ]), method = &quot;euclidean&quot;) ## 1 2 3 4 5 ## 2 0.2276858 ## 3 2.0933591 2.0527035 ## 4 1.1846152 1.0686434 1.0912475 ## 5 0.2979638 0.1195600 1.9389490 0.9491006 ## 6 1.5021428 1.4204061 3.3998698 2.3248142 1.5121727 dist(unclass(add_lr_switch[1:6, ]), method = &quot;euclidean&quot;) ## 1 2 3 4 5 ## 2 0.3486014 ## 3 2.1713475 1.9078113 ## 4 1.6405267 1.3254326 0.7286134 ## 5 0.4820630 0.1465561 1.7668173 1.1788856 ## 6 0.9461828 0.8883672 2.6176853 1.9278663 0.9708201 Les statistiques linéaires sont toutefois les mêmes peu importe le dénominateur commun: elles seront seulement exprimées selon les axes définis par la composition de référence (dénominateur commum). Une stratégie proposée par Aitchison qui permet la mesure de distances euclidienne est d’effectuer un log-ratio entre chaque composante et la moyenne géométrique de toutes les composantes. Cette transformation se nomme le log-ratio centré (\\(clr\\), pour centered log-ratio) \\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\] En R, cen_lr &lt;- clr(comp) Avec des CLRs, les distances sont valides. Mais… nous restons avec le problème de la redondance d’information. En fait, la somme de chacunes des lignes d’une matrice de clr est de 0. Pas très pratique lorsque l’on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, géostatistiques, etc.) cen_lr %&gt;% cov() %&gt;% solve() Error in solve.default(.) : le système est numériquement singulier : conditionnement de la réciproque = 4.44407e-17 Enfin, une autre méthode de transformation développée par Egoscue et al. (2003), les log-ratios isométriques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormées. Ces dimensions doivent doivent être préalablement établie dans un dendrogramme de bifurcation, où chaque composante ou groupe de composante est successivement divisé en deux embranchement. La manière d’arranger ces balances importe peu, mais on aura avantage à créer des balances interprétables. Le diagramme de balances peut être encodé dans une partition binaire séquentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contrastes orthogonaux ou chaque ligne représente une partition entre deux variables ou groupes de variables. Une composante étiquettée +1 correspondra au groupe du numérateur, une composante étiquettée -1 au dénominateur et une composante étiquettée 0 sera exclue de la partition (Parent et al., 2013). J’ai reformulé la fonction CoDaDendrogram pour que l’on puisse ajouter des informations intéressantes sur les balants horizontaux. Cette fonction est disponible sur github. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) sbp &lt;- matrix(c(1, 1,-1, 1,-1, 0), byrow = TRUE, ncol = 3) CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1), equal.height = TRUE, type = &quot;boxplot&quot;, group = cut(ArcticLake$depth, 3)) Figure 1.6: Dendrogramme compositionnel: de la surface (rouge) vers la profondeur (bleu). Si la SBP est plus imposante (la précédente n’a que trois composantes, mais en pratique il y en a souvent davantage), il pourrait être plus aisé de monter dans un chiffrier, puis de l’importer dans R via un fichier csv. Le calcul des ILRs est effectué comme suit. \\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\] où, à la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au numérateur et au dénominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne géométrique des composantes au numérateur et \\(g \\left( c_j^- \\right)\\) est la moyenne géométrique des composantes au dénominateur. Les balances sont conventionnellement notées [A,B | C,D], ou les composantes A et B au dénominateur sont balancées avec les composantes C and D au numérateur. Une balance positive signifie que la moyenne géométrique des concentrations au numérateur est supérieur à celle au dénominateur, et inversement, alors qu’une balance nulle signifie que les moyennes géométriques sont égales (équilibre). Ainsi, en modélisation linéaire, un coefficient positif sur [A,B | C,D] signifie que l’augmentation de l’importance de C et D comparativement à A et B est associé à une augmentation de la variable réponse du modèle. En R, iso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp))) Notez la forme gsi.buildilrBase(t(sbp)) est une opération pour obtenir la matrice d’orthonormalité à partir de la SBP. Les ILRs sont des balances multivariées sur lesquelles on pourra effectuer des statistiques linéaries. Bien que l’interprétation des résultats comme collection d’interprétations sur des balances univariées pourra être affectée par la structure de la SBP, ni les statistiques linéaires multivariées, ni la distance entre les points ne seront affectés. En effet, chaque variante de la SBP est une rotation (d’un facteur de 60°) par rapport à l’origine: source(&quot;lib/ilr-rotation-sbp.R&quot;) Figure 1.7: Rotation des axes obtenus selon trois SBP différentes. Pour les transformations inverses, c’est-à-dire pour passer de données transformées à des données exprimées dans l’échelle originale, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez à garder la trace de vos données dans leur format original, vous aurez avantage à ajouter à votre vecteur compositionnel la valeur de remplissage, constitué d’un amalgame des composantes non mesurées. Par exemple, pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) acomp(pourc) # vous perdez la trace des proportions originales ## N P K ## 0.73170732 0.02439024 0.24390244 ## attr(,&quot;class&quot;) ## [1] acomp pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) Fv &lt;- 1 - sum(pourc) comp &lt;- acomp(c(pourc, Fv = Fv)) comp ## N P K Fv ## 0.030 0.001 0.010 0.959 ## attr(,&quot;class&quot;) ## [1] acomp iso_lr &lt;- ilr(comp) # avec une sbp par défaut ilrInv(iso_lr) ## 1 2 3 4 ## [1,] 0.03 0.001 0.01 0.959 ## attr(,&quot;class&quot;) ## [1] acomp 1.4 Les zéros Les proportions négatives ou plus imposantes que le tout sont exclues. De même, une proportion de 100%, occupant tout le simplex, implique que les proportions complémentaires soient nulles. Or, le log de 0 est \\(-\\infty\\), ce qui dispose de la possibilité d’effectuer des transformations logarithmiques. En fait, les données compositionnelles sont exprimées entre 0 et la somme totale, ce qui exclut la possibilité d’obtenir des proportions de 0% ou de 100%. Or, il arrive souvent qu’un tableau de données inclut des 0. Dans ce cas, on doit mener une réflexion sur la nature des valeurs nulles. 1.4.1 Les zéros arrondis Arrondissez 0.01 à une seule décimale et vous obtiendrez 0.0. Ou bien, mesurez une concentration de 0.01 avec un appareil dont le seuil de détection est 0.1, vous obtiendrez encore 0.0. Étant des valeurs qui ne peuvent être observées pour des raisons méthodologiques, les zéros arrondis sont équivalents à des valeurs manquantes non aléatoires (Martin-Fernandez et al., 2011). Il existe plusieurs manières de remplacer ces zéros par une valeur attendue. La méthode la plus simple est de remplacer les zéros par une proportion du seuil de détection, par exemple 65% (Martin-Fernandez et al., 2003). Chargeons d’abord des données fictives. library(&quot;zCompositions&quot;) set.seed(542353) data(LPdata) rows &lt;- sample(1:nrow(LPdata), 3) LPdata[rows, ] ## Cr B P V Cu Ti Ni Y Sr La Ce Ba Li K Rb ## 38 27.5 17 148 29 2.7 4335 0.0 17 24 8 163 74 30 9703 48 ## 3 30.4 23 433 42 3.8 3305 16.6 22 59 14 240 75 80 12209 53 ## 79 25.6 14 135 33 0.0 3925 14.2 20 18 9 97 36 77 3319 41 Si le seuil de détection est de 1 pour toutes les colonnes, LPdata_z &lt;- LPdata LPdata_z[LPdata == 0] &lt;- 1*0.65 LPdata_z[rows, ] ## Cr B P V Cu Ti Ni Y Sr La Ce Ba Li K Rb ## 38 27.5 17 148 29 2.70 4335 0.65 17 24 8 163 74 30 9703 48 ## 3 30.4 23 433 42 3.80 3305 16.60 22 59 14 240 75 80 12209 53 ## 79 25.6 14 135 33 0.65 3925 14.20 20 18 9 97 36 77 3319 41 Par exemple, le remplacement multiplicatif simple incluse dans le module zCompositions, dont la mathématique varie légèrement. LPdata_z &lt;- multRepl(LPdata, dl = rep(1, ncol(LPdata)), delta = 0.65, label = 0) LPdata_z[rows, ] ## Cr B P V Cu Ti Ni Y Sr La Ce Ba Li K Rb ## 38 27.5 17 148 29 2.7000000 4335 0.6500289 17 24 8 163 74 30 9703 48 ## 3 30.4 23 433 42 3.8000000 3305 16.6000000 22 59 14 240 75 80 12209 53 ## 79 25.6 14 135 33 0.6500544 3925 14.2000000 20 18 9 97 36 77 3319 41 1.4.2 Les décomptes de zéro Vous décomptez des espèces sur une parcelle ou bien séquencez des OTU (operational taxonomy units, qui peuvent être interprétés - avec une précaution bioinformaticienne - comme des espèces): certaines espèces ne seront pas observées, bien que leur occurence puisse être attendue. Si vous vous intéressez davantage aux proportions d’espèces qu’à leur décompte absolu, vous devriez transformer vos données avec les technique présentées ci-dessus. De même que précédemment, la non observation d’un décompte peut provienir de contraintes méthodologiques: parcelle trop petite, temps d’observation trop court, ou quoi qui pourrait incluencer la non observation d’une fréquence rare. Par exemple, les décomptes de cochons occupés à différentes activités. set.seed(30147) data(Pigs) rows &lt;- sample(1:nrow(Pigs), 3) Pigs[rows, ] ## BED HALF.BED PASSAGE HALF.PASS FEEDER HALF.FEED ## 19 83 1 9 1 3 0 ## 15 68 1 16 1 10 1 ## 18 69 0 26 0 2 0 En ces circonstances, il est possible de remplacer les zéros par des méthodes bayésiennes. Les données sont du coup transformées en proportions. Pigs_z &lt;- cmultRepl(Pigs, method = &quot;CZM&quot;) # CZM: count zero multiplicative Pigs_z[rows, ] ## BED HALF.BED PASSAGE HALF.PASS FEEDER HALF.FEED ## 19 0.8528032 0.010274737 0.09247263 0.010274737 0.03082421 0.003350515 ## 15 0.7010309 0.010309278 0.16494845 0.010309278 0.10309278 0.010309278 ## 18 0.7041901 0.003350515 0.26534701 0.003350515 0.02041131 0.003350515 Dans le cas des OTU, il arrive souvent d’obtenir au-dessus de 90% de zéros. Une option est de regrouper (amalgamer) autant que possible les OTU en taxons d’ordre supérieurs. 1.4.3 Les zéros structurels Il peut arriver que les zéros soient générés par les mécanismes étudiés, non pas des erreurs de mesure. Par exemple, certaines espèces peuvent ne pas occuper certaines aires cadrillées parce qu’elles sont structurellement exclues de leur niche écologique (une épinette dans un lac ou la proportion de viande au menu d’une famille véganne). Pour ce type de zéro, il n’y a pas de réponse facile. De même que précédemment, il pourrait être possible de procéder à l’amalgamation. Une méthode conviviale est de séparer l’analyse en différents groupes (par exemple, on décompte généralement des espèces différentes sur terre et dans un lac). Si vos données font partie d’un tout, je vous recommande chaudement d’utiliser des méthodes compositionnelles autant pour l’analyse que la modélisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado (2013) pourra vous guider, quoi que je recommande davantage le plus récent et à mon avis mieux vulgarisé [Applied Compositional Data Analysis With Worked Examples in R], de Filzmoser et al. (2018). "],
["les-biostats-compositionnelles.html", "2 Les biostats compositionnelles", " 2 Les biostats compositionnelles "],
["autoapprentissage-compositionnel.html", "3 Autoapprentissage compositionnel 3.1 Lexique 3.2 Démarche 3.3 En résumé, 3.4 L’autoapprentissage en R 3.5 Les k plus proches voisins 3.6 Les processus gaussiens", " 3 Autoapprentissage compositionnel On peut lier les données compositionnelles avec une ou plusieurs autres à l’aide de régressions linéaires, polynomiales, sinusoïdales, exponentielle, sigmoïdales, etc. Encore faut-il s’assurer que ces formes préétablies représentent le phénomène de manière fiable. Lorsque la forme de la réponse est difficile à envisager, en particulier dans des cas non-linéaires ou impliquant plusieurs variables, on pourra faire appel à des modèles dont la structure n’est pas contrôlée par une équation rigide gouvernée par des paramètres (comme la pente ou l’intercept). L’autoapprentissage vise à détecter des structures complexes émergeant d’ensembles de données à l’aide des mathématiques et de processus automatisés afin de prédire l’émergence de futures occurrences. 3.1 Lexique L’autoapprentissage possède son jargon particulier. Les variables réponse sont celles que nous comptons prédire, alors que les variables d’entrée sont utilisées pour prédire une réponse. Alors que l’apprentissage supervisé inclue une variable réponse, l’apprentissage non-supervisé, lui, n’en a pas: il est surtout utilisé pour créer des catégories à partir de données qui ne sont pas préalablement étiquettées. Les apprentissages par régression prédisent des variables continuent alors que les apprentissage par classification prédisent des catégories. Les données d’entraînement servent à ajuster le modèle alors que les données de test servent à évaluer sa performance. 3.2 Démarche 3.2.1 Prétraitement Pour la plupart des techniques d’autoapprentissage, le choix de l’échelle de mesure est déterminant sur la modélisation subséquente. Par exemple, un algorithme basé sur la distance comme les k plus proches voisins ne mesurera pas les mêmes distances entre deux observations si l’on change l’unité de mesure d’une variable du mètre au kilomètre. En outre, les transformations compositionnelles sont une forme de prétraitement. 3.2.2 Entraînement et test Un modèle prut fonctionner très bien en terrain connu, mais une prédiction doit être évaluée sur des données pour lesquelles le modèle ne connait pas la réponse. En pratique, il faut séparer un tableau de données en deux: un tableau d’entraînement et un tableau de test. Il n’existe pas de standards sur la proportion à utiliser dans l’un et l’autre. Rarement, toutefois, réservera-t-on moins plus de 50% et moins de 20% à la phase de test. Dans tous les cas, on doit porter une attention particulière à l’équilibre des données dans chaque tableau (les distributions devraient être semblables). L’analyste doit s’assurer de séparer le tableau au hasard pour ne pas biaiser le modèle, mais de manière consciencieuse pour éviter d’évacuer des cas rares dans un tableau ou dans l’autre. 3.2.3 Validation croisée Lorsque l’on considère une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interprète du bruit comme une structure, on est en cas de surapprentissage. Une manière de limiter le mésapprentissage est d’avoir recours à la validation croisée. La validation croisée est un principe incluant plusieurs algorithmes qui consiste à entraîner le modèle sur un échantillonnage aléatoire des données d’entraînement. La technique la plus utilisée est le k-fold, où l’on sépare aléatoirement le tableau d’entraînement en un nombre k de tableaux. À chaque étape de la validation croisée, on calibre le modèle sur tous les tableaux sauf un, puis on valide le modèle sur le tableau exclu. 3.2.4 Choix de l’algorithme d’apprentissage Choisir l’algorithme (ou les algorithmes) adéquats pour votre problème n’est pas une tâche facile. Ce choix sera motivé par le problème à régler, les tenants et aboutissants des algorithmes, votre expérience, l’expérience de la littérature, l’expérience de vos collègues, etc. Ce serait peu productif d’étudier la mathématique de chacun d’eux. Une approche raisonnable est de tester plusieurs modèles, de retenir les modèles qui semblent les plus pertinents, et d’approfondir si ce n’est déjà fait la mathématique des options retenues et d’apprendre à les maîtriser au fil de vos expériences. Le module scikit-learn, qui fonctionne en langage Python, propose néanmoins un schéma décisionnel. Figure 3.1: Schéma décisionnel des algorithmes pertinents. Source: scikit-learn. 3.2.5 Déploiement Nous ne couvrirons pas la phase de déploiement d’un modèle. Notons seulement qu’il est possible, en R, d’exporter un modèle dans un fichier .Rdata, qui pourra être chargé dans un autre environnement R. Cet environnement peut être une feuille de calcul comme une interface visuelle montée par exemple avec Shiny. 3.3 En résumé, Explorer les données Sélectionner des algorithmes Effectuer un prétraitement Créer un ensemble d’entraînement et un ensemble de test Lisser les données sur les données d’entraînement avec validation croisée Tester le modèle Déployer le modèle 3.4 L’autoapprentissage en R Plusieurs options sont disponibles. Je vais introduire le module caret de R a été conçu pour donner accès à des centaines de fonctions d’autoapprentissage via une interface commune. library(&quot;caret&quot;) Pour cet atelier, nous allons couvrir les k plus proches voisins et les processus gaussiens. 3.5 Les k plus proches voisins Le principe des KNN (k-nearest neighbors: un objet va ressembler à ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une métrique de distance pour rechercher un nombre k de points situés à proximité de la mesure. Les k points les plus proches sont retenus, k étant un entier non nul à optimiser. Un autre paramètre parfois utilisé est la distance maximale des voisins à considérer: un voisin trop éloigné pourra être discarté. La réponse attribuée à la mesure est calculée à partir de la réponse des k voisins retenus. Dans le cas d’une régression, on utiliser généralement la moyenne. Dans le cas de la classification, la mesure prendra la catégorie qui sera la plus présente chez les k plus proches voisins. La métrique de distance devient importante et une standardisation des données (par exemple soustraire la moyenne, puis diviser par l’écart-type) est généralement nécessaire en prétraitement. Pour ce qui est des données compositionnelles, on pourra tirer profit de la métrique d’Aitchison en calculant les distances euclidienne sur des données transformées par clr ou ilr. 3.5.1 Exemple d’application L’ionome foliaire est la concentration en éléments d’une feuille: ce sont des données compositionnelles. Nous allons prédire une espèce fictive à partir de son ionome seulement (si d’autres variables étaient intégrées à la prédiction, il faudrait standardiser les données). veggies &lt;- read_csv(&quot;data/legumes_fictifs.csv&quot;) head(veggies) ## # A tibble: 6 x 7 ## Culture Annee N P K Ca Mg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PanetRave 2007 3.13 0.36 5.33 1.96 0.36 ## 2 PanetRave 2007 3.03 0.37 4.94 2.33 0.4 ## 3 PanetRave 2007 3.01 0.36 5.08 1.76 0.36 ## 4 PanetRave 2007 3.53 0.36 5.44 2.13 0.37 ## 5 PanetRave 2007 3.96 0.38 6.1 2.76 0.5 ## 6 PanetRave 2007 3.9 0.38 6.06 2.66 0.48 Effectuons un prétraitement compositionnel sous forme d’ilr. veggies &lt;- veggies %&gt;% mutate(Fv = 100 - (N+P+K+Ca+Mg)) sbp &lt;- matrix(c(1, 1, 1, 1, 1,-1, 1, 1,-1,-1,-1, 0, 1,-1, 0, 0, 0, 0, 0, 0, 1,-1,-1, 0, 0, 0, 0, 1,-1, 0), ncol = 6, byrow = TRUE) veggies_ilr &lt;- veggies %&gt;% dplyr::select(N, P, K, Ca, Mg, Fv) %&gt;% ilr(., V = gsi.buildilrBase(t(sbp))) %&gt;% as_tibble() %&gt;% mutate(Culture = veggies$Culture) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. Séparons les données en entraînement (_tr) et en test (_te) en utilisant la fonction caret::createDataPartition avec une proportion 70/30 (p = 0.7). Il est essentiel d’utiliser set.seed() pour s’assurer que la partition soit la même à chaque session de code (pour la reproductibilité) - j’ai l’habitude de taper n’importe quel numéro à environ 6 chiffres, mais lors de publications, je vais sur random.org et je génère un numéro au hasard, sans biais. set.seed(68017) id_tr &lt;- createDataPartition(veggies_ilr$Culture, p = 0.7, list = FALSE) veggies_tr &lt;- veggies_ilr[id_tr, ] veggies_te &lt;- veggies_ilr[-id_tr, ] L’objet id_tr comprend les indices de ligne des données d’entraînement. Avant de lancer nos calculs, allons vois sur la page de caret les modules qui effectuent des KNN pour la classification. Nous trouvons knn et kknn. Prenons kknn. Nous pourrions utiliser une grille de paramètre pour l’optimisation du modèle, mais laissons caret générer une grille par défaut. Nous allons néamoins modéliser avec une validation croisée à 5 plis. ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) Pour finalement lisser le modèle. set.seed(8961704) clf &lt;- train(Culture ~ ., data = veggies_tr, method = &quot;kknn&quot;, trainControl = ctrl) clf ## k-Nearest Neighbors ## ## 342 samples ## 5 predictor ## 4 classes: &#39;ChouMarseille&#39;, &#39;Courgine&#39;, &#39;PanetRave&#39;, &#39;PommeMer&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 342, 342, 342, 342, 342, 342, ... ## Resampling results across tuning parameters: ## ## kmax Accuracy Kappa ## 5 0.9738582 0.9647107 ## 7 0.9738582 0.9647107 ## 9 0.9738582 0.9647107 ## ## Tuning parameter &#39;distance&#39; was held constant at a value of 2 ## ## Tuning parameter &#39;kernel&#39; was held constant at a value of optimal ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were kmax = 9, distance = 2 and ## kernel = optimal. Nous obtenons les paramètres du modèle optimal. Prédisons l’espèce selon son ionome pour chacun des tableaux. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = veggies_te) Une manière d’évaluer la prédiction est d’afficher un tableau de contingence. table(Observed = veggies_tr$Culture, Predicted = pred_tr) ## Predicted ## Observed ChouMarseille Courgine PanetRave PommeMer ## ChouMarseille 67 0 0 0 ## Courgine 0 95 0 0 ## PanetRave 0 0 101 0 ## PommeMer 0 0 0 79 table(Observed = veggies_te$Culture, Predicted = pred_te) ## Predicted ## Observed ChouMarseille Courgine PanetRave PommeMer ## ChouMarseille 27 0 1 0 ## Courgine 0 40 0 0 ## PanetRave 3 0 39 0 ## PommeMer 0 0 0 33 Les espèces sont toutes classées en entraînement, mais quelques rares erreurs surviennent en test. 3.6 Les processus gaussiens Les prédictions que nous avons obtenues des KNN sont des catégories, mais on aurait pu aussi prédire des nombres réels. Dans les cas où la crédibilité de la réponse est importante, il devient pertinent que la sortie soit probabiliste: les prédictions seront alors présentées sous forme de distributions de probabilité. Les processus gaussiens snt en mesure de prédire des distributions (prédictions probabilistes). Le principe des processus gaussiens est, à l’aide d’une matrice de covariance qui doit être définie (que l’on nomme le noyau), de générer une distribution multinormale, puis d’en sortir une distribution conditionnée sur les observations. Le processus est semblable à la modélisation de la variance courrament utilisée en géostatistiques. Pour plus de détails, référez-vous aux notes du cours Anayse et modélisation d’agroécosystèmes. Retenez seulement que (1) la matrice de covariance a une influence capitale et que (2) vous devez standardiser (centrer et réduire) vos entrées et vos réponses. 3.6.1 Exemple d’application Les données de pollution en métaux lourds sur une rive de la rivière Meuse, en France, sont souvent utilisées pour les exemples d’application en prédiction spatiale. Nous allons utiliser les processus gaussiens (avec le module kernlab) pour prédire les ilr des métaux lourds à partir des coordonnées et de la distance de la rivière, en profitant de l’occasion pour cartographier avec les modules ggmap et sf. Notez que les ilr sont ici les variables-réponse, alors qu’ils étaient les variables d’entrée dans l’exemple précédent avec les KNN. library(&quot;ggmap&quot;) library(&quot;sf&quot;) meuse &lt;- read_csv(&quot;data/meuse.csv&quot;) meuse %&gt;% head() ## # A tibble: 6 x 14 ## x y cadmium copper lead zinc elev dist om ffreq soil ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181072 333611 11.7 85 299 1022 7.91 0.00136 13.6 1 1 ## 2 181025 333558 8.6 81 277 1141 6.98 0.0122 14 1 1 ## 3 181165 333537 6.5 68 199 640 7.8 0.103 13 1 1 ## 4 181298 333484 2.6 81 116 257 7.66 0.190 8 1 2 ## 5 181307 333330 2.8 48 117 269 7.48 0.277 8.7 1 2 ## 6 181390 333260 3 61 137 281 7.79 0.364 7.8 1 2 ## # ... with 3 more variables: lime &lt;dbl&gt;, landuse &lt;chr&gt;, dist.m &lt;dbl&gt; Les coordonnées sont exprimées en format néerlandais EPSG:28992. Transformons-les en longitudes et latitudes sur l’ellipsoïde NAD83 avec le module sf. meuse_geo &lt;- meuse %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 28992) %&gt;% st_transform(&quot;+proj=longlat +datum=NAD83&quot;) meuse_coord &lt;- meuse_geo %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% mutate(cadmium = meuse$cadmium, copper = meuse$copper, lead = meuse$lead, zinc = meuse$zinc, dist = meuse$dist) %&gt;% rename(x = &quot;X&quot;, y = &quot;Y&quot;) meuse_coord %&gt;% head() ## # A tibble: 6 x 7 ## x y cadmium copper lead zinc dist ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.76 51.0 11.7 85 299 1022 0.00136 ## 2 5.76 51.0 8.6 81 277 1141 0.0122 ## 3 5.76 51.0 6.5 68 199 640 0.103 ## 4 5.76 51.0 2.6 81 116 257 0.190 ## 5 5.76 51.0 2.8 48 117 269 0.277 ## 6 5.76 51.0 3 61 137 281 0.364 À partir des coordonnées, nous pouvons grâce à ggmap::get_stamenmap effectuer une requête pour télécherger les tuiles d’un fond de carte, sur lesquels nous affichons nous concentrations en zinc. meuse_map &lt;- get_stamenmap(bbox = c(left = 5.716, right = 5.767, bottom = 50.95, top = 51), zoom = 14, maptype = &quot;terrain&quot;) ggmap(meuse_map) + geom_point(data = meuse_coord, aes(x=x, y=y, fill = zinc), shape = 21, size = 2) + scale_fill_viridis_c(option = &quot;inferno&quot;, direction = -1) + theme_bw() Figure 3.2: Mesures de concentration en zinc dans les sols d’une rive de la rivière Meuse Transformons maintenant nos concentrations en ilr. sbp &lt;- matrix(c(1, 1, 1, 1,-1, 1, 1,-1,-1, 0, 1,-1, 0, 0, 0, 0, 0, 1,-1, 0), ncol = 5, byrow = TRUE) psi &lt;- gsi.buildilrBase(t(sbp)) meuse_ilr &lt;- meuse %&gt;% mutate(Fv = 1e6 - cadmium - copper - lead - zinc) %&gt;% dplyr::select(cadmium, copper, lead, zinc, Fv) %&gt;% acomp(.) %&gt;% ilr(., V = psi) %&gt;% as_tibble() names(meuse_ilr) &lt;- c(&quot;CdCuPbZn.Fv&quot;, &quot;CdCu.PbZn&quot;, &quot;Cd.Cu&quot;, &quot;Pb.Zn&quot;) meuse_ilr %&gt;% head() ## # A tibble: 6 x 4 ## CdCuPbZn.Fv CdCu.PbZn Cd.Cu Pb.Zn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -7.99 -2.86 -1.40 -0.869 ## 2 -8.06 -3.06 -1.59 -1.00 ## 3 -8.37 -2.83 -1.66 -0.826 ## 4 -8.86 -2.48 -2.43 -0.562 ## 5 -8.94 -2.73 -2.01 -0.589 ## 6 -8.83 -2.67 -2.13 -0.508 Ces données sont jumelées avec les coordonnées et la distance de la rivière pour créer un tableau prêt pour l’autoapprentissage. meuse_ml &lt;- meuse_coord %&gt;% dplyr::select(x, y, dist) %&gt;% bind_cols(meuse_ilr) Puis, séparons les données. set.seed(1046584) train_id &lt;- createDataPartition(meuse$y, p=0.7, list=FALSE)# #pour obtenir un équilibre sur les latitudes meuse_tr &lt;- meuse_ml[train_id, ] meuse_te &lt;- meuse_ml[-train_id, ] Bien que l’autoapprentissage sera effectué avec caret, vous aurez avantage à prendre davantage de contrôle sur les processus gaussiens en utilisant directement la fonction kernlab::gausspr(), qui permettra du coup de prédire l’incertitude de la prédiction. Notez toutefois que R ne possède toujours pas de module polyvalent pour les processus gaussiens, ce qui me motive habituellement à basculer en Python à cette étape. Continuons néanmoins avec R, en sélectionnant un noyau de type radial que nous ne tenterons pas d’optimiser et sur lesquels nous laissons tomber la validation croisée pour éviter de compliquer l’exemple. Enfin, notez les données sont dans ce cas-ci mises automatiquement à l’échelle. set.seed(4896378) mod_CdCuPbZn.Fv &lt;- train(CdCuPbZn.Fv ~ x + y + dist, data = meuse_tr, method = &quot;knn&quot;) mod_CdCu.PbZn &lt;- train(CdCu.PbZn ~ x + y + dist, data = meuse_tr, method = &quot;gaussprRadial&quot;) mod_Cd.Cu &lt;- train(Cd.Cu ~ x + y + dist, data = meuse_tr, method = &quot;gaussprRadial&quot;) mod_Pb.Zn&lt;- train(Pb.Zn ~ x + y + dist, data = meuse_tr, method = &quot;gaussprRadial&quot;) Effectuons la série de prédictions en remettant le tout dans l’échelle originale. pred1_tr &lt;- predict(mod_CdCuPbZn.Fv) pred1_te &lt;- predict(mod_CdCuPbZn.Fv, newdata = meuse_te) pred2_tr &lt;- predict(mod_CdCu.PbZn) pred2_te &lt;- predict(mod_CdCu.PbZn, newdata = meuse_te) pred3_tr &lt;- predict(mod_Cd.Cu) pred3_te &lt;- predict(mod_Cd.Cu, newdata = meuse_te) pred4_tr &lt;- predict(mod_Pb.Zn) pred4_te &lt;- predict(mod_Pb.Zn, newdata = meuse_te) Nous pouvons évaluer notre modèle en comparant les prédictions. par(mfrow = c(4, 2)) plot(meuse_tr$CdCuPbZn.Fv, pred1_tr, main = &quot;Train&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_te$CdCuPbZn.Fv, pred1_te, main = &quot;Test&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_tr$CdCu.PbZn, pred2_tr, main = &quot;Train&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_te$CdCu.PbZn, pred2_te, main = &quot;Test&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_tr$Cd.Cu, pred3_tr, main = &quot;Train&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_te$Cd.Cu, pred3_te, main = &quot;Test&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_tr$Pb.Zn, pred4_tr, main = &quot;Train&quot;) ; abline(0, 1, col=&quot;red&quot;) plot(meuse_te$Pb.Zn, pred4_te, main = &quot;Test&quot;) ; abline(0, 1, col=&quot;red&quot;) Figure 3.3: Évaluation visuelle de la prédiction spatiale par processus gaussien Les résultats montrent que les modèles ne sont pas exceptionnels, et qu’une optimisation critique et conscientieuse devrait être effectuée en vue d’obtenir de meilleures prédictions. Continuons néanmoins. La prédiction spatiale demande une grille comprenant des points sur lesquels on voudra effectuer des prédictions. Il est possible d’en créer une de différentes manières en R ou dans des systèmes d’information géographique. Dans ce cas, elle est disponible en format csv. meuse_grid &lt;- read_csv(&quot;data/meuse_grid.csv&quot;) meuse_grid %&gt;% head() ## # A tibble: 6 x 7 ## x y part.a part.b dist soil ffreq ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181180 333740 1 0 0 1 1 ## 2 181140 333700 1 0 0 1 1 ## 3 181180 333700 1 0 0.0122 1 1 ## 4 181220 333700 1 0 0.0435 1 1 ## 5 181100 333660 1 0 0 1 1 ## 6 181140 333660 1 0 0.0122 1 1 meuse_grid %&gt;% ggplot(aes(x=x, y=y)) + coord_fixed() + geom_point(size=0.1) + theme_bw() Figure 3.4: Grille pour la prédiction spatiale. Ramenons les coordonnées sur une même référence que celles utilisées pour la prédiction. meuse_grid_coord &lt;- meuse_grid %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = 28992) %&gt;% st_transform(&quot;+proj=longlat +datum=NAD83&quot;) %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% mutate(dist = meuse_grid$dist) %&gt;% rename(x = &quot;X&quot;, y = &quot;Y&quot;) Effectuons les prédictions des ilr sur la grille. pred1 &lt;- predict(mod_CdCuPbZn.Fv, newdata = meuse_grid_coord) pred2 &lt;- predict(mod_CdCu.PbZn, newdata = meuse_grid_coord) pred3 &lt;- predict(mod_Cd.Cu, newdata = meuse_grid_coord) pred4 &lt;- predict(mod_Pb.Zn, newdata = meuse_grid_coord) Ces prédictions sont consignées dans un tableau. meuse_pred &lt;- meuse_grid_coord %&gt;% as_tibble() %&gt;% mutate(CdCuPbZn.Fv = pred1, CdCu.PbZn = pred2, Cd.Cu = pred3, Pb.Zn = pred4) Ce tableau est utiliser pour effectuer la transformation inverse de l’ilr à la composition. # Transformation inverse meuse_pred_comp &lt;- meuse_pred %&gt;% dplyr::select(CdCuPbZn.Fv, CdCu.PbZn, Cd.Cu, Pb.Zn) %&gt;% ilrInv(., V=psi) # En ppm meuse_pred_comp &lt;- unclass(meuse_pred_comp) * 1e6 # En format tibble avec les bons noms meuse_pred_comp &lt;- meuse_pred_comp %&gt;% as_tibble() %&gt;% rename(&quot;cadmium&quot; = &quot;V1&quot;, &quot;copper&quot; = &quot;V2&quot;, &quot;lead&quot; = &quot;V3&quot;, &quot;zinc&quot; = &quot;V4&quot;, &quot;Fv&quot; = &quot;V5&quot;) ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. # Fusionner avec le tableau de prédiction meuse_pred_comp &lt;- bind_cols(meuse_pred, meuse_pred_comp) Enfin, nous pouvons projeter les résultats sur notre carte pour chaque point de la grille. gg_cd &lt;- ggmap(meuse_map) + geom_point(data = meuse_pred_comp, aes(x=x, y=y, colour = cadmium)) + geom_point(data = meuse_coord, aes(x=x, y=y, size = cadmium), shape = 1) + scale_colour_viridis_c(option = &quot;inferno&quot;, direction = -1) + theme_bw() gg_cu &lt;- ggmap(meuse_map) + geom_point(data = meuse_pred_comp, aes(x=x, y=y, colour = copper)) + geom_point(data = meuse_coord, aes(x=x, y=y, size = copper), shape = 1) + scale_colour_viridis_c(option = &quot;inferno&quot;, direction = -1) + theme_bw() gg_pb &lt;- ggmap(meuse_map) + geom_point(data = meuse_pred_comp, aes(x=x, y=y, colour = lead)) + geom_point(data = meuse_coord, aes(x=x, y=y, size = lead), shape = 1) + scale_colour_viridis_c(option = &quot;inferno&quot;, direction = -1) + theme_bw() gg_zn &lt;- ggmap(meuse_map) + geom_point(data = meuse_pred_comp, aes(x=x, y=y, colour = zinc)) + geom_point(data = meuse_coord, aes(x=x, y=y, size = zinc), shape = 1) + scale_colour_viridis_c(option = &quot;inferno&quot;, direction = -1) + theme_bw() cowplot::plot_grid(gg_cd, gg_cu, gg_pb, gg_zn, ncol = 2) Figure 3.5: Prédiction spatiale des métaux lourds sur une rive de la rivière Meuse. "],
["liens-utiles.html", "4 Liens utiles", " 4 Liens utiles Diagrammes ternaires ggtern. soilTexture Cours d’Analyse et modélisation d’agroécosystèmes (GAE-7007, Université Laval) "]
]
